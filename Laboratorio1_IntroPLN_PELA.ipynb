{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'freeling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-e3edb2b4532a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfreeling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'freeling'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk import classify\n",
    "from sklearn import cross_validation\n",
    "import re\n",
    "import codecs\n",
    "import freeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importamos el corpus de comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comentarios = pandas.read_csv(\"comentarios_peliculas.csv\", skiprows=1, delimiter=';', skip_blank_lines=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Realizamos un breve analisis del corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El corpus tiene 1447 comentarios\n",
      "Hay 859 comentarios positivos lo que equivale al  59.3%\n",
      "Hay 222 comentarios neutros lo que equivale al  15.3%\n",
      "Hay 366 comentarios negativos lo que equivale al  25.2%\n",
      "El largo de palabras promedio de los comentarios es 38\n"
     ]
    }
   ],
   "source": [
    "\n",
    "positivos = 0 \n",
    "neutros = 0\n",
    "negativos = 0\n",
    "total = str(len(comentarios))\n",
    "largo_comentario = 0\n",
    "lista_comentarios = comentarios.values.tolist()\n",
    "for x in lista_comentarios:\n",
    "    if x[7] < 3:\n",
    "        negativos += 1\n",
    "    elif x[7] == 3:\n",
    "        neutros += 1\n",
    "    else:\n",
    "        positivos += 1\n",
    "    largo_comentario += len(re.findall(r'\\w+', x[6]))\n",
    "\n",
    "\n",
    "avg_positivos = str(float(positivos)*100 / float(total))\n",
    "avg_negativos = str(float(negativos)*100 / float(total))\n",
    "avg_neutros = str(float(neutros)*100 / float(total))\n",
    "avg_largo_comentario = str(float(largo_comentario)/ float(total))\n",
    "        \n",
    "print (\"El corpus tiene \" + total + \" comentarios\")\n",
    "print (\"Hay \" + str(positivos) + \" comentarios positivos lo que equivale al  \" + avg_positivos[:4] + \"%\")\n",
    "print (\"Hay \" + str(neutros) + \" comentarios neutros lo que equivale al  \" + avg_neutros[:4] + \"%\")\n",
    "print (\"Hay \" + str(negativos) + \" comentarios negativos lo que equivale al  \" + avg_negativos[:4] + \"%\")\n",
    "print (\"El largo de palabras promedio de los comentarios es \" + avg_largo_comentario[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Separamos entrenamiento y testeo, lo pasamos a listas comunes y eliminamos datos que no utilizamos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El corpus de entrenamiento tiene largo 1147\n",
      "El corpus de testing tiene largo 300\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) \n",
    "msk = np.random.rand(len(comentarios)) < 0.8\n",
    "comentarios_training= comentarios[msk] \n",
    "comentarios_testing = comentarios[~msk] \n",
    "\n",
    "lista_comentarios_training = comentarios_training.values.tolist()\n",
    "for x in lista_comentarios_training:\n",
    "    del x[8]\n",
    "    del x[5]\n",
    "    del x[4]\n",
    "    del x[3]\n",
    "    del x[2]\n",
    "    del x[1]\n",
    "    del x[0]\n",
    "print (\"El corpus de entrenamiento tiene largo \" + str(len(lista_comentarios_training)))\n",
    "lista_comentarios_testing = comentarios_testing.values.tolist()\n",
    "for x in lista_comentarios_testing:\n",
    "    del x[8]\n",
    "    del x[5]\n",
    "    del x[4]\n",
    "    del x[3]\n",
    "    del x[2]\n",
    "    del x[1]\n",
    "    del x[0]\n",
    "print (\"El corpus de testing tiene largo \" + str(len(lista_comentarios_testing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Obtenemos las palabras mas comunes del corpus, quitando puntuación y stopwords del Español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textoComentariosTraining = \"\"\n",
    "for x in lista_comentarios_training:\n",
    "    textoComentariosTraining += \" \" + x[0].lower()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "corpus_text = tokenizer.tokenize(textoComentariosTraining) \n",
    "stopwordsEspanol = nltk.corpus.stopwords.words('spanish')\n",
    "stopwordsEspanol.remove(\"no\")\n",
    "stopwordsEspanol.remove(\"sí\")\n",
    "palabrasMasComunes = FreqDist(w.lower() for w in corpus_text if w not in stopwordsEspanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Utilizamos cross-validation para determinar el mejor N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.255\n",
      "             2          -0.81646        0.591\n",
      "             3          -0.80877        0.591\n",
      "             4          -0.80135        0.591\n",
      "             5          -0.79420        0.591\n",
      "             6          -0.78732        0.591\n",
      "             7          -0.78072        0.592\n",
      "             8          -0.77438        0.593\n",
      "             9          -0.76830        0.597\n",
      "         Final          -0.76248        0.605\n",
      "accuracy: 0.6331877729257642\n",
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.251\n",
      "             2          -0.80537        0.597\n",
      "             3          -0.79726        0.597\n",
      "             4          -0.78948        0.597\n",
      "             5          -0.78203        0.597\n",
      "             6          -0.77490        0.597\n",
      "             7          -0.76809        0.599\n",
      "             8          -0.76159        0.600\n",
      "             9          -0.75540        0.606\n",
      "         Final          -0.74950        0.612\n",
      "accuracy: 0.5807860262008734\n",
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.251\n",
      "             2          -0.80537        0.597\n",
      "             3          -0.79726        0.597\n",
      "             4          -0.78948        0.597\n",
      "             5          -0.78203        0.597\n",
      "             6          -0.77490        0.597\n",
      "             7          -0.76809        0.599\n",
      "             8          -0.76159        0.600\n",
      "             9          -0.75540        0.606\n",
      "         Final          -0.74950        0.612\n",
      "accuracy: 0.7368421052631579\n",
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.251\n",
      "             2          -0.80537        0.597\n",
      "             3          -0.79726        0.597\n",
      "             4          -0.78948        0.597\n",
      "             5          -0.78203        0.597\n",
      "             6          -0.77490        0.597\n",
      "             7          -0.76809        0.599\n",
      "             8          -0.76159        0.600\n",
      "             9          -0.75540        0.606\n",
      "         Final          -0.74950        0.612\n",
      "accuracy: 0.5131578947368421\n",
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.252\n",
      "             2          -0.79616        0.601\n",
      "             3          -0.78749        0.601\n",
      "             4          -0.77917        0.601\n",
      "             5          -0.77121        0.601\n",
      "             6          -0.76361        0.602\n",
      "             7          -0.75635        0.602\n",
      "             8          -0.74943        0.604\n",
      "             9          -0.74284        0.611\n",
      "         Final          -0.73657        0.614\n",
      "accuracy: 0.5789473684210527\n",
      "[0.608584233509538]\n"
     ]
    }
   ],
   "source": [
    "def word_feats(commentWords): # FUNCION QUE DEVUELVE UN DICCIONARIO DE FEATURES, CON VALORES TRUE Y FALSE\n",
    "    commentWords = tokenizer.tokenize(commentWords)\n",
    "    return dict([(word[0], word[0] in commentWords) for word in palabras_features])\n",
    "\n",
    "def classification(value): # FUNCION QUE TRANSFORMA UN NUMERO DE 1..5 A UNA CLASIFICACION\n",
    "    if value > 3:\n",
    "        classification = \"negative\"\n",
    "    elif value == 3:\n",
    "        classification = \"neutro\"\n",
    "    else:\n",
    "        classification = \"positive\"\n",
    "    return classification\n",
    "\n",
    "def crossValidation(array): # Funcion que calcula la accuracy dado cierto array de entrenamiento, \n",
    "                            # que es un array, de array de features y su clasificacion\n",
    "    k = 5 # Por defecto, se utiliza 10\n",
    "    max_iterations = 10 \n",
    "    totalAccuracy = 0\n",
    "    cv = cross_validation.KFold(len(array), k, None, False)\n",
    "    for traincv, testcv in cv: \n",
    "        classifier = nltk.MaxentClassifier.train(array[traincv[0]:traincv[len(traincv)-1]], max_iter=max_iterations) \n",
    "        accuracyCV = nltk.classify.util.accuracy(classifier, array[testcv[0]:testcv[len(testcv)-1]])\n",
    "        print ('accuracy:', accuracyCV)\n",
    "        totalAccuracy += accuracyCV\n",
    "    return (totalAccuracy/k)\n",
    "\n",
    "# VARIABLES DEL FOR. DETERMINAN EL TIEMPO DE EJECUCION EN GRAN MEDIDA\n",
    "posiblesN = [40] \n",
    "\n",
    "listaAccuracy = [] \n",
    "\n",
    "for n in posiblesN:\n",
    "    palabras_features = palabrasMasComunes.most_common(n)\n",
    "    array_for_training = [(word_feats(f[0]), classification(f[1])) for f in lista_comentarios_training]\n",
    "    accuracyN = crossValidation(array_for_training)\n",
    "    listaAccuracy.append(accuracyN)\n",
    "    \n",
    "print(listaAccuracy)\n",
    "\n",
    "# SELECCIONAMOS EL MEJOR N\n",
    "index = listaAccuracy.index(max(listaAccuracy)) # OBTENGO INDICE DEL MAYOR\n",
    "N = posiblesN[index]\n",
    "\n",
    "#para ver la accuracy contra testing, cosa que NO hay que hacer:::\n",
    "#array_for_test = [(word_feats(f[0]), classification(f[1])) for f in lista_comentarios_testing]\n",
    "#accuracyTesting = classify.accuracy(bestClassifierCV,array_for_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El mejor N encontrado es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5. Incluimos la lista de elementos subjetivos como features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_feats_eS(commentWords):\n",
    "    commentWords = tokenizer.tokenize(commentWords)\n",
    "    valor = valor_comentario_eS(commentWords)\n",
    "    array =  dict([(word[0], word[0] in commentWords) for word in palabras_features_eS])\n",
    "    array[\"Valor\"] = valor\n",
    "    return array\n",
    "\n",
    "def valor_comentario_eS(commentWords):\n",
    "    suma = 0\n",
    "    for word in commentWords:\n",
    "        if word in elementos_subjetivos_positivos:  \n",
    "            suma += 1\n",
    "        elif word in elementos_subjetivos_negativos:\n",
    "            suma -= 1        \n",
    "    if suma > 0:\n",
    "        return 1\n",
    "    elif suma < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0    \n",
    "\n",
    "def get_valor(linea):\n",
    "    q = re.sub('\\(\\'.*\\'\\,','',linea)\n",
    "    r = re.sub('\\)','',q)\n",
    "    return r\n",
    "\n",
    "def get_palabra(linea):\n",
    "    q = re.sub('\\(\\'','',linea)\n",
    "    r = re.sub('\\'\\,.*\\)','',q)\n",
    "    return r\n",
    "\n",
    "elementos_subjetivos_positivos = []\n",
    "elementos_subjetivos_negativos = []\n",
    "with codecs.open('elementos_subjetivos.txt','r',encoding='utf8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    for l in lines:\n",
    "        if \"elementoSubjetivo\" in l and \"%\" not in l:\n",
    "            p = re.sub('elementoSubjetivo','',l)\n",
    "            q = re.sub('\\.','',p)\n",
    "            if get_valor(q) == \"3\":\n",
    "                elementos_subjetivos_positivos.append(get_palabra(q))\n",
    "            else:\n",
    "                elementos_subjetivos_negativos.append(get_palabra(q))\n",
    "\n",
    "palabras_features_eS = palabrasMasComunes.most_common(N)\n",
    "array_for_training_eS = [(word_feats_eS(f[0]), classification(f[1])) for f in lista_comentarios_training]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Ahora calculamos qué tan bueno es nuestro nuevo clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.255\n",
      "             2          -0.81475        0.591\n",
      "             3          -0.80558        0.591\n",
      "             4          -0.79676        0.591\n",
      "             5          -0.78829        0.591\n",
      "             6          -0.78015        0.591\n",
      "             7          -0.77236        0.593\n",
      "             8          -0.76490        0.596\n",
      "             9          -0.75777        0.602\n",
      "         Final          -0.75095        0.602\n",
      "accuracy: 0.62882096069869\n",
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.251\n",
      "             2          -0.80369        0.597\n",
      "             3          -0.79417        0.597\n",
      "             4          -0.78504        0.597\n",
      "             5          -0.77631        0.597\n",
      "             6          -0.76798        0.599\n",
      "             7          -0.76004        0.600\n",
      "             8          -0.75247        0.601\n",
      "             9          -0.74527        0.605\n",
      "         Final          -0.73842        0.608\n",
      "accuracy: 0.5807860262008734\n",
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.251\n",
      "             2          -0.80369        0.597\n",
      "             3          -0.79417        0.597\n",
      "             4          -0.78504        0.597\n",
      "             5          -0.77631        0.597\n",
      "             6          -0.76798        0.599\n",
      "             7          -0.76004        0.600\n",
      "             8          -0.75247        0.601\n",
      "             9          -0.74527        0.605\n",
      "         Final          -0.73842        0.608\n",
      "accuracy: 0.7280701754385965\n",
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.251\n",
      "             2          -0.80369        0.597\n",
      "             3          -0.79417        0.597\n",
      "             4          -0.78504        0.597\n",
      "             5          -0.77631        0.597\n",
      "             6          -0.76798        0.599\n",
      "             7          -0.76004        0.600\n",
      "             8          -0.75247        0.601\n",
      "             9          -0.74527        0.605\n",
      "         Final          -0.73842        0.608\n",
      "accuracy: 0.5087719298245614\n",
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.252\n",
      "             2          -0.79457        0.601\n",
      "             3          -0.78460        0.601\n",
      "             4          -0.77504        0.601\n",
      "             5          -0.76592        0.601\n",
      "             6          -0.75721        0.602\n",
      "             7          -0.74891        0.604\n",
      "             8          -0.74101        0.605\n",
      "             9          -0.73350        0.606\n",
      "         Final          -0.72636        0.612\n",
      "accuracy: 0.5833333333333334\n",
      "0.605956485099211\n"
     ]
    }
   ],
   "source": [
    "print(crossValidation(array_for_training_eS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
